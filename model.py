import numpy as np
from tensorflow.python.keras.models import Sequential
from tensorflow.python.keras.layers import Dense, Activation
from tensorflow.python.keras.layers import LSTM
from tensorflow.python.keras.optimizers import RMSprop
from tensorflow.python.keras.callbacks import LambdaCallback, ModelCheckpoint, ReduceLROnPlateau
import sys
import random

# Begin by processing the data to create a dataset that we will use to train our model
# Data processing parameters
sequence_length = 40 # The length of the sequence the model will use to make a prediction on
step_size = 3 # How much to step by when creating the next training sequence (greater step
              # implies less sequences and each one is more unique due to less overlap)

# Load the cleaned txt file of the aggregrated transcripts of the podcast called Practical AI
# (cleaned txt was generated by collectdata.py)
with open('all_transcripts_cleaned.txt', 'r') as file:
    # Simplify model by reducing tokens to only lowercase letters
    text = file.read().lower()
print('text length', len(text))

# Convert text data into numbers by creating a set of unique characters that
# are present in the textual data, and mapping characters to indices and vice-versa
chars = sorted(list(set(text))) # getting all unique chars
print('total chars: ', len(chars))

# Mapping
char_indices = dict((c, i) for i, c in enumerate(chars))
indices_char = dict((i, c) for i, c in enumerate(chars))

# Split the textual data into sequences of size sequence_length, and record what the next character
# is. This will be used to train the model to predict the next character based on the previous 
# sequence_length character/tokens.
sentences = []
next_chars = []
for i in range(0, len(text) - sequence_length, step_size):
    # Create a sequence of all characters from i to (i + sequence_length - 1)
    sentences.append(text[i: i + sequence_length])
    # Record the next character after the sequence (the character text[i + sequence_length]) as
    # the character that should be predicted by our RNN if our input into the RNN was the sequence
    next_chars.append(text[i + sequence_length])

# Convert these lists into one-hot encodings in numpy
x = np.zeros((len(sentences), sequence_length, len(chars)), dtype=bool)
y = np.zeros((len(sentences), len(chars)), dtype=bool)
for i, sentence in enumerate(sentences):
    for t, char in enumerate(sentence):
        x[i, t, char_indices[char]] = 1
    y[i, char_indices[next_chars[i]]] = 1

# Create the model architecture
model = Sequential()
model.add(LSTM(128, input_shape=(sequence_length, len(chars))))
model.add(Dense(len(chars)))
model.add(Activation('softmax'))

optimizer = RMSprop(learning_rate=0.01)
model.compile(loss='categorical_crossentropy', optimizer=optimizer)

# Helper functions for training
def sample(preds, temperature=1.0):
    # helper function to sample an index from a probability array
    preds = np.asarray(preds).astype('float64')
    preds = np.log(preds) / temperature
    exp_preds = np.exp(preds)
    preds = exp_preds / np.sum(exp_preds)
    probas = np.random.multinomial(1, preds, 1)
    return np.argmax(probas)

def on_epoch_end(epoch, logs):
    # Function invoked at end of each epoch. Prints generated text.
    print()
    print('----- Generating text after Epoch: %d' % epoch)

    start_index = random.randint(0, len(text) - sequence_length - 1)
    for diversity in [0.2, 0.5, 1.0, 1.2]:
        print('----- diversity:', diversity)

        generated = ''
        sentence = text[start_index: start_index + sequence_length]
        generated += sentence
        print('----- Generating with seed: "' + sentence + '"')
        sys.stdout.write(generated)

        for i in range(400):
            x_pred = np.zeros((1, sequence_length, len(chars)))
            for t, char in enumerate(sentence):
                x_pred[0, t, char_indices[char]] = 1.

            preds = model.predict(x_pred, verbose=0)[0]
            next_index = sample(preds, diversity)
            next_char = indices_char[next_index]

            generated += next_char
            sentence = sentence[1:] + next_char

            sys.stdout.write(next_char)
            sys.stdout.flush()
        print()

# Create callback functions to do the following during training: monitor performance, save progress,
# and reduce learning rate as the model plateaus.
print_callback = LambdaCallback(on_epoch_end=on_epoch_end)
filepath = "weights.hdf5"
checkpoint = ModelCheckpoint(filepath, monitor='loss',
                             verbose=1, save_best_only=True,
                             mode='min')
reduce_lr = ReduceLROnPlateau(monitor='loss', factor=0.2,
                              patience=1, min_lr=0.001)

callbacks = [print_callback, checkpoint, reduce_lr]

# Train the model
model.fit(x, y, batch_size=128, epochs=5, callbacks=callbacks)

def generate_text(length, diversity):
    # Get random starting text
    start_index = random.randint(0, len(text) - sequence_length - 1)
    generated = ''
    sentence = text[start_index: start_index + sequence_length]
    generated += sentence
    for i in range(length):
            x_pred = np.zeros((1, sequence_length, len(chars)))
            for t, char in enumerate(sentence):
                x_pred[0, t, char_indices[char]] = 1.

            preds = model.predict(x_pred, verbose=0)[0]
            next_index = sample(preds, diversity)
            next_char = indices_char[next_index]

            generated += next_char
            sentence = sentence[1:] + next_char
    return generated

print(generate_text(1000, 0.2))